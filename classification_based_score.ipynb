{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df212d7f-209b-47d5-ae23-9d8dd3dcb4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "018ba02b-e769-4d32-9488-b9174be9554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Jiann/STORAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12875409-ef9e-4ec5-a005-ff470139852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "moral = []\n",
    "moral += ds[\"storal_en_train\"]\n",
    "moral += ds[\"storal_en_test\"]\n",
    "moral += ds[\"storal_en_valid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f165125a-c600-41ab-aa34-07ade01b3958",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "import random\n",
    "\n",
    "for x in range(len(ds[\"storal_en_train\"])):\n",
    "    train_x.append(ds[\"storal_en_train\"][x]['story'] + \"the moral of this story is:\" + ds[\"storal_en_train\"][x]['moral'])\n",
    "    train_y.append(1)\n",
    "  \n",
    "for x in range(len(ds[\"storal_en_train\"])):\n",
    "\n",
    "    i = random.sample(range(1, len(ds[\"storal_en_train\"])), 2)\n",
    "    \n",
    "    train_x.append(ds[\"storal_en_train\"][i[0]]['story'] + \"the moral of this story is:\" + ds[\"storal_en_train\"][i[1]]['moral'])\n",
    "    train_y.append(0)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb96200e-969c-4c4f-b3be-fbf79da6d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = []\n",
    "test_y = []\n",
    "\n",
    "for x in range(len(ds[\"storal_en_test\"])):\n",
    "    test_x.append(ds[\"storal_en_test\"][x]['story'] + \"the moral of this story is:\" + ds[\"storal_en_test\"][x]['moral'])\n",
    "    test_y.append(1)\n",
    "  \n",
    "for x in range(len(ds[\"storal_en_test\"])):\n",
    "\n",
    "    i = random.sample(range(1, len(ds[\"storal_en_test\"])), 2)\n",
    "    \n",
    "    test_x.append(ds[\"storal_en_test\"][i[0]]['story'] + \"the moral of this story is:\" + ds[\"storal_en_test\"][i[1]]['moral'])\n",
    "    test_y.append(0)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "639d9b85-43e6-4526-a0ce-4b5609b0f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "data_dict_list = [\n",
    "    {\"text\": t, \"label\": l} for t, l in zip(train_x, train_y)\n",
    "]\n",
    "\n",
    "# 3. Create the Hugging Face Dataset from the list of dictionaries\n",
    "dataset = Dataset.from_list(data_dict_list)\n",
    "\n",
    "data_dict_list = [\n",
    "    {\"text\": t, \"label\": l} for t, l in zip(test_x, test_y)\n",
    "]\n",
    "\n",
    "# 3. Create the Hugging Face Dataset from the list of dictionaries\n",
    "test_dataset = Dataset.from_list(data_dict_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2227ecc-bfd6-4fe9-981f-2d48601cf0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|█████████████████████████| 2136/2136 [00:01<00:00, 1215.10 examples/s]\n",
      "Map: 100%|███████████████████████████| 712/712 [00:00<00:00, 1142.95 examples/s]\n",
      "/home/kuroro/py_envs/lib/python3.12/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/kuroro/py_envs/lib/python3.12/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='402' max='402' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [402/402 3:44:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=402, training_loss=0.6946937219420476, metrics={'train_runtime': 13500.9416, 'train_samples_per_second': 0.475, 'train_steps_per_second': 0.03, 'total_flos': 1686015642746880.0, 'train_loss': 0.6946937219420476, 'epoch': 3.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Trainer API for fine-tuning\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_test_dataset\n",
    ")\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4d6e4b5-bc18-49d4-9081-268b8416b240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.5034279823303223}]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = trainer.model\n",
    "# Create a pipeline for text classification\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "# Test the classifier on some text\n",
    "result = classifier(\"RoBERTa is an amazing language model!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "09a9a596-fd9c-4ef3-bc6a-57d73a15d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = []\n",
    "test_y = []\n",
    "\n",
    "for x in range(len(ds[\"storal_en_test\"])):\n",
    "    test_x.append(ds[\"storal_en_test\"][x]['story'] + \"the moral of this story is:\" + ds[\"storal_en_test\"][x]['moral'])\n",
    "    test_y.append(1)\n",
    "  \n",
    "for x in range(len(ds[\"storal_en_test\"])):\n",
    "\n",
    "    i = random.sample(range(1, len(ds[\"storal_en_test\"])), 2)\n",
    "    \n",
    "    test_x.append(ds[\"storal_en_test\"][i[0]]['story'] + \"the moral of this story is:\" + ds[\"storal_en_test\"][i[1]]['moral'])\n",
    "    test_y.append(0)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8b154d5b-4e10-4ad8-bc1e-d26998e1a498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list = 0\n",
    "\n",
    "for i in range(len(test_x)):\n",
    "    if len(test_x[i]) > 511:\n",
    "        test_x[i] = test_x[i][:511]\n",
    "    r = classifier(test_x[i])[0]['score']\n",
    "    if r > 0.5 and test_y[i] == 1:\n",
    "        result_list += 1\n",
    "    elif r < 0.5 and test_y[i] == 0:\n",
    "        result += 1\n",
    "\n",
    "result_list/len(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a3ae5-0faf-4c7e-8c2c-0fa9a7972d73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
